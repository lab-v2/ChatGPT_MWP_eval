{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1067,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pandas\n",
    "import numpy as numpy\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# These are the columns the program will extract mathematical features from.\n",
    "# FORMAT: You can list multiple columns by separating the names with a comma\n",
    "COLUMNS_WITH_EQUATIONS = \"lEquations\"\n",
    "\n",
    "# When generating the geq columns, this constant will define the number of geq columns generated for each math feature\n",
    "NUM_OF_SYMBOL_MAX_VALUE = 8\n",
    "\n",
    "CORRECT_COLUMN = \"is_correct\"\n",
    "VALID_COLUMN = \"valid\"\n",
    "\n",
    "# These are just constants which define what sort of values we're looking for and the column which indicates whether a row is valid\n",
    "CORRECT_COLUMN = \"is_correct\"\n",
    "VALID_COLUMN = \"valid\"\n",
    "\n",
    "QUESTION_NO = 'question_No'\n",
    "\n",
    "# Some string consts so the column names can be modified a bit easier\n",
    "NUM_OF_ADDITION_SUFFIX = \"_num_of_addition\"\n",
    "NUM_OF_SUBTRACTION_SUFFIX = \"_num_of_subtraction\"\n",
    "NUM_OF_ADDITION_AND_SUBTRACTION_SUFFIX = \"_num_of_addition_and_subtraction\"\n",
    "NUM_OF_MULTIPLICATION_SUFFIX = \"_num_of_multiplication\"\n",
    "NUM_OF_DIVISION_SUFFIX = \"_num_of_division\"\n",
    "NUM_OF_MULTIPLICATION_AND_DIVISION_SUFFIX = \"_num_of_multiplication_and_division\"\n",
    "NUM_OF_EQUATIONS_SUFFIX = \"_num_of_equations\"\n",
    "\n",
    "\n",
    "COLUMN_TO_CHECK = \"result\"\n",
    "\n",
    "MAX_NAME_DIFFERENCE = 3\n",
    "\n",
    "# Various constants with text defined by Abhinav\n",
    "# These constants are here so that, if Abhinav changes his mind on what the strings will look like, \n",
    "# the information is centralized\n",
    "ALL_ANSWERS = \"has all the answers\"\n",
    "ALL_ANSWERS_ROUNDED = \"has all the answers when rounded\"\n",
    "SOME_SOLUTION = \"has one or more of the answers, but not all of them\"\n",
    "SOME_SOLUTION_ROUNDED = \"has one or more of the answers when rounded, but not all of them\"\n",
    "NO_SOLUTION = \"says no solution\"\n",
    "INVALID = \"invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_symbol(equations, symbol):\n",
    "    # Calculates the number of '+' symbols in [equations]\n",
    "    # INPUT: [equations] should be an array of strings which represent equations\n",
    "    # INPUT: [symbol] should be a character.\n",
    "    count = 0\n",
    "    for equation in equations:\n",
    "        count += equation.count(symbol)\n",
    "    return count\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "def num_of_addition(equations):\n",
    "    # Calculates the number of '+' symbols in [equations]\n",
    "    # INPUT: [equations] should be an array of strings which represent equations\n",
    "    return num_of_symbol(equations, '+')\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def num_of_subtraction(equations): \n",
    "    # Calculates the number of '-' symbols in [equations]\n",
    "    # INPUT: [equations] should be an array of strings which represent equations\n",
    "    return num_of_symbol(equations, '-')\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "def num_of_multiplication(equations):\n",
    "    # Calculates the number of '*' symbols in [equations]\n",
    "    # INPUT: [equations] should be an array of strings which represent equations\n",
    "    return num_of_symbol(equations, '*')\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "def num_of_division(equations):\n",
    "    # Calculates the number of '/' symbols in [equations]\n",
    "    # INPUT: [equations] should be an array of strings which represent equations\n",
    "    return num_of_symbol(equations, '/')\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "def num_of_equations(equations):\n",
    "    # Calculates the number of equations in [equations]\n",
    "    # INPUT: [equations] should be an array of strings which represent equations\n",
    "    return len(equations)\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "def generate_geq_columns(data, column, max_value):\n",
    "    # Generates greater than or equal binary columns of a mathematical feature.\n",
    "    # Once we calculate the number (amount) of each mathematical feature, \n",
    "    # we generate columns of 0s and 1s representing whether the equations have greater than or equal\n",
    "    # number of a particular feature\n",
    "    for value in range(1, max_value + 1):\n",
    "        data[column + \"_geq_\" + str(value)] = generate_geq_column(data, column, value)\n",
    "    return data\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "def generate_geq_column(data, column, value):\n",
    "    # Generates a column of 0s and 1s which represents whether or not the rows in data[column] are >= value. 1 means True and 0 means False \n",
    "    return data.apply(lambda row : 1 if row[column] >= value else 0, axis=1)\n",
    "    \n",
    "    \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(input_file_path, output_file_path):\n",
    "    # Load data from files\n",
    "    data = pandas.read_json(input_file_path)\n",
    "\n",
    "    # Calculate the number (amount) of each particular mathematical features\n",
    "    # We calculate the number of additions (+), subtractions (-), multiplications (*), divisions (*) and equations.\n",
    "    columns_with_equations = COLUMNS_WITH_EQUATIONS.split(',')\n",
    "    for column in columns_with_equations:\n",
    "        data[column + NUM_OF_ADDITION_SUFFIX] = data.apply(lambda row : num_of_addition(row[column]), axis=1)\n",
    "        data[column + NUM_OF_SUBTRACTION_SUFFIX] = data.apply(lambda row : num_of_subtraction(row[column]), axis=1)\n",
    "        data[column + NUM_OF_MULTIPLICATION_SUFFIX] = data.apply(lambda row : num_of_multiplication(row[column]), axis=1)\n",
    "        data[column + NUM_OF_DIVISION_SUFFIX] = data.apply(lambda row : num_of_division(row[column]), axis=1)\n",
    "        data[column + NUM_OF_EQUATIONS_SUFFIX] = data.apply(lambda row : num_of_equations(row[column]), axis=1)\n",
    "        \n",
    "        data[column + NUM_OF_ADDITION_AND_SUBTRACTION_SUFFIX] = data[column + NUM_OF_ADDITION_SUFFIX] + data[column + NUM_OF_SUBTRACTION_SUFFIX]\n",
    "        data[column + NUM_OF_MULTIPLICATION_AND_DIVISION_SUFFIX] = data[column + NUM_OF_MULTIPLICATION_SUFFIX] + data[column + NUM_OF_DIVISION_SUFFIX]\n",
    "\n",
    "        # data = generate_geq_columns(data, column + NUM_OF_ADDITION_SUFFIX, NUM_OF_SYMBOL_MAX_VALUE)\n",
    "        # data = generate_geq_columns(data, column + NUM_OF_SUBTRACTION_SUFFIX, NUM_OF_SYMBOL_MAX_VALUE)\n",
    "        data = generate_geq_columns(data, column + NUM_OF_ADDITION_AND_SUBTRACTION_SUFFIX, NUM_OF_SYMBOL_MAX_VALUE)\n",
    "\n",
    "        # data = generate_geq_columns(data, column + NUM_OF_MULTIPLICATION_SUFFIX, NUM_OF_SYMBOL_MAX_VALUE)\n",
    "        # data = generate_geq_columns(data, column + NUM_OF_DIVISION_SUFFIX, NUM_OF_SYMBOL_MAX_VALUE)\n",
    "        data = generate_geq_columns(data, column + NUM_OF_MULTIPLICATION_AND_DIVISION_SUFFIX, NUM_OF_SYMBOL_MAX_VALUE)\n",
    "\n",
    "        data = generate_geq_columns(data, column + NUM_OF_EQUATIONS_SUFFIX, NUM_OF_SYMBOL_MAX_VALUE)\n",
    "\n",
    "    # Save data to output file\n",
    "    data.to_json(output_file_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation(column):\n",
    "    # negation -- \n",
    "    # OUTPUT: returns a column of 0s and 1s of the negation of [column]. 1s are flipped to 0 and vice versa\n",
    "    # INPUT: [column] should be a column of 0s and 1s\n",
    "    return 1 - column\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def conjunction(column_1, column_2):\n",
    "    # conjunction -- \n",
    "    # output: returns a column of 0s and 1s of the conjunction between [column_1] and [column_2].\n",
    "    # INPUT: [column_1] and [column_2] should be columns of 0s and 1s\n",
    "    return column_1 * column_2\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def disjunction(column_1, column_2):\n",
    "    # disjunction -- \n",
    "    # OUTPUT: returns a column of 0s and 1s of the disjunction between [column_1] and [column_2].\n",
    "    # INPUT: [column_1] and [column_2] should be columns of 0s and 1s\n",
    "    return column_1 | column_2\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def conditional_probability(occurence_column, condition_column):\n",
    "    # conditional_probability -- \n",
    "    # OUTPUT: returns a number which represents the conditional probability p(occurence | condition)\n",
    "    # INPUT: [occurence_column] and [condition_column] should be columns of 0s and 1s\n",
    "    return conjunction(occurence_column, condition_column).sum() / condition_column.sum()\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def prior(data):\n",
    "    # prior -- \n",
    "    # OUTPUT: returns a number which represents the prior\n",
    "    # INPUT: [data] should be a Pandas dataframe with the columns [CORRECT_COLUMN] and [VALID_COLUMN].\n",
    "    # TODO : Possible optimizations can be made where we cache the result instead of calling this expensive operation again and again\n",
    "    return conditional_probability(data[CORRECT_COLUMN], data[VALID_COLUMN])\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def is_prima_facie(data, column_name):\n",
    "    # is_prima_facie -- \n",
    "    # OUTPUT: returns a boolean which determines whether the column indicated by [column_name] is a prima facie\n",
    "    # INPUT: [data] should be a Pandas dataframe with the columns [CORRECT_COLUMN] and [VALID_COLUMN].\n",
    "    # INPUT: [column_name] should be a valid column in [data]\n",
    "    # INPUT: The [CORRECT_COLUMN] and [VALID_COLUMN] columns should be columns of 0s and 1s \n",
    "    return conditional_probability(data[CORRECT_COLUMN], data[column_name]) > prior(data)\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def is_cooccur(column_1, column_2):\n",
    "    # is_cooccur -- \n",
    "    # OUTPUT: returns a boolean based on if there is at least one row where both [column_1] and [column_2] is equal to 1\n",
    "    # INPUT: [column_1] and [column_2] should both be columns of 0s and 1s\n",
    "    return conjunction(column_1, column_2).sum() > 0\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def is_same_category(column_name_1, column_name_2):\n",
    "    # same_category -- \n",
    "    # OUTPUT: Returns a boolean signifying whether the [column_name_1] and [column_name_2] are different by [MAX_NAME_DIFFERENCE]\n",
    "    #         If the two words are not different by [MAX_NAME_DIFFERENCE], they are in the same category so it returns true\n",
    "    count = 0\n",
    "    shortest = min(len(column_name_1), len(column_name_2))\n",
    "    for i in range(0, shortest):\n",
    "        if column_name_1[i] == column_name_2[i]:\n",
    "            count = count + 1\n",
    "    return count < MAX_NAME_DIFFERENCE\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def rel(data, column_name):\n",
    "    # rel -- \n",
    "    # OUTPUT: returns a list of the names of other columns which cooccur with [column_name] and are prima facie\n",
    "    # INPUT: [data] should be a Pandas dataframe with the columns [CORRECT_COLUMN] and [VALID_COLUMN].\n",
    "    # INPUT: [column_name] should be a valid column in [data]\n",
    "    # INPUT: The [CORRECT_COLUMN] and [VALID_COLUMN] columns should be columns of 0s and 1s \n",
    "    # If it is not a prima facie cause, we don't bother to find its rel\n",
    "    if not is_prima_facie(data,column_name): return[]\n",
    "    \n",
    "    name_list = []\n",
    "    for potential_cause in data.columns:\n",
    "        # Make sure we are not including the [CORRECT_COLUMN] and [VALID_COLUMN] as part of rel\n",
    "        if potential_cause == CORRECT_COLUMN or potential_cause == VALID_COLUMN:\n",
    "            continue\n",
    "\n",
    "        if is_same_category(potential_cause, column_name): continue\n",
    "\n",
    "        if is_cooccur(data[column_name], data[potential_cause]) and is_prima_facie(data, potential_cause):\n",
    "            name_list.append(potential_cause)\n",
    "    return name_list\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def calculate_causality(data, column_name):\n",
    "    # calculate_causality -- \n",
    "    # OUTPUT: returns a number which represents the causality value of the column indicated by [column_name]\n",
    "    # INPUT: [data] should be a Pandas dataframe with the columns [CORRECT_COLUMN].\n",
    "    # INPUT: [column_name] should be a valid column in [data]\n",
    "    # INPUT: The [CORRECT_COLUMN] and [VALID_COLUMN] columns should be columns of 0s and 1s \n",
    "\n",
    "    # If it's not a prima facie cause, we don't bother to calculate its causality value\n",
    "    if not is_prima_facie(data, column_name):\n",
    "        return \"n/a\"\n",
    "\n",
    "    relateds = rel(data, column_name)\n",
    "\n",
    "    total_probability = 0\n",
    "    for related in relateds:\n",
    "        conj = conjunction(data[column_name], data[related])\n",
    "        negj = conjunction(negation(data[column_name]), data[related])\n",
    "\n",
    "        conj = conditional_probability(data[CORRECT_COLUMN], conj)\n",
    "        negj = conditional_probability(data[CORRECT_COLUMN], negj)\n",
    "\n",
    "        total_probability += (conj - negj)\n",
    "\n",
    "    if (len(relateds) > 0): return total_probability / len(relateds)\n",
    "    else: return 0\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def is_binary_column(data, column_name):\n",
    "    # is_binary_column --\n",
    "    # Checks to see if a column is a column of 1s and 0s\n",
    "    # INPUT: [data] is a dataframe\n",
    "    # INPUT: [column_name] should be the name of a valid column in [data]\n",
    "    return data.apply(lambda row : 0 if (isinstance(row[column_name], int) and (row[column_name] <= 1)) else 1, axis=1).sum() <= 0\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def remove_non_binary_columns(data):\n",
    "    # remove_non_binary_columns --\n",
    "    # Removes all columns that are not 0s or 1s in the dataset\n",
    "    # INPUT: [data] is a dataframe\n",
    "    non_binary = []\n",
    "    for i in data.columns:\n",
    "        if not is_binary_column(data, i):\n",
    "            non_binary.append(i)\n",
    "\n",
    "    return data.drop(columns=non_binary)\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "def generate_row(data, column_name):\n",
    "    # generate_row --\n",
    "    # TODO: This is kind of a terrible name but I can't really think of anything more descriptive. If anyone has any ideas, feel free to modify it\n",
    "    # It basically creates a row, which is actually a data frame with all the data that is needed\n",
    "    # OUTPUT: It outputs a row with all the required values\n",
    "    # INPUT: [data] should be a dataframe\n",
    "    # INPUT: [column_name] should be a string representing a valid column in [data]\n",
    "    toReturn = pandas.DataFrame({\n",
    "        \"name\": [column_name], \n",
    "        \"support\": conjunction(data[column_name], data[VALID_COLUMN]).sum(),\n",
    "        \"causality\": calculate_causality(data, column_name),\n",
    "        \"rel\": ','.join(rel(data, column_name)),\n",
    "        \"conditional_probability\":[conditional_probability(data[CORRECT_COLUMN], data[column_name])], \n",
    "        \"prior\": prior(data),\n",
    "        \"conditional - prior\": conditional_probability(data[CORRECT_COLUMN], data[column_name]) - prior(data)\n",
    "    })\n",
    "    return toReturn\n",
    "    \n",
    "    \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(combined_output_file_path, chatgpt_file_path, output_file_path, is_correct, is_valid):\n",
    "    problems = pandas.read_json(combined_output_file_path)\n",
    "    chatgpt = pandas.read_json(chatgpt_file_path)\n",
    "\n",
    "    for column in chatgpt.columns:\n",
    "        if column == QUESTION_NO:\n",
    "            continue\n",
    "\n",
    "        problems.loc[chatgpt[QUESTION_NO], column] = chatgpt[column]\n",
    "\n",
    "    problems[CORRECT_COLUMN] = problems.apply(lambda row : is_correct(row),axis=1)\n",
    "    problems[VALID_COLUMN] = problems.apply(lambda row : is_valid(row),axis=1)\n",
    "\n",
    "    problems.to_json(output_file_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causality_values(input_file_path, output_file_path):\n",
    "    # causality_values --\n",
    "    # Calculates causality values\n",
    "\n",
    "    # Load data\n",
    "    data = pandas.read_json(input_file_path)\n",
    "\n",
    "    # Then remove all the non binary columns\n",
    "    data = remove_non_binary_columns(data)\n",
    "\n",
    "    # TODO: I'm not sure if there's another way to do this, so feel free to make modifications\n",
    "    # Generate a dud data frame with a single so we can append to it.\n",
    "    to_save = generate_row(data, VALID_COLUMN)\n",
    "    for column in data.columns:\n",
    "        if column == VALID_COLUMN or column == CORRECT_COLUMN:\n",
    "            continue\n",
    "\n",
    "        to_save = to_save.append(generate_row(data, column))\n",
    "\n",
    "    # Remove the dud first row\n",
    "    to_save = to_save[1:]\n",
    "\n",
    "    to_save.to_json(output_file_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_stats(input_file_path, output_file_path):\n",
    "    data = pandas.read_json(input_file_path)\n",
    "    count = data['result'].value_counts().rename_axis('value').reset_index(name='count')\n",
    "    count.to_json(output_file_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(input_file_path, output_file_path, percent_correct):\n",
    "    data = pandas.read_json(input_file_path)\n",
    "\n",
    "    correct_data = data[data[CORRECT_COLUMN] == 1]\n",
    "    wrong_data = data[data[CORRECT_COLUMN] == 0]\n",
    "\n",
    "    correct_len = min(len(correct_data.index), len(wrong_data.index))\n",
    "    wrong_len = int(correct_len / percent_correct * (1 - percent_correct))\n",
    "\n",
    "    correct_data = correct_data.head(correct_len)\n",
    "    wrong_data = wrong_data.head(wrong_len)\n",
    "\n",
    "    correct_data = correct_data.append(wrong_data)\n",
    "    correct_data = correct_data.sample(frac = 1, random_state=42)\n",
    "\n",
    "    correct_data.to_json(output_file_path, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(name, input_file_path, output_file_path):\n",
    "    data = pandas.read_json(input_file_path)\n",
    "    data = remove_non_binary_columns(data)\n",
    "    \n",
    "    data_y = data[CORRECT_COLUMN]\n",
    "    data_x = data.drop(columns=[CORRECT_COLUMN, VALID_COLUMN])\n",
    "\n",
    "    output =  pandas.DataFrame({\n",
    "        \"fold\": [-1],\n",
    "        \"guessed \" + name  + \" precision score\": [0],\n",
    "        \"guessed \" + name  + \" recall score\": [0],\n",
    "        \"guessed not \" + name  + \" precision score\": [0],\n",
    "        \"guessed not \" + name  + \" recall score\": [0],\n",
    "    })\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "    ])\n",
    "    # Split the data set into training and test set using Stratified Sampling\n",
    "    split = StratifiedKFold(n_splits=5,random_state=RANDOM_STATE, shuffle=True)\n",
    "    current_fold = 0\n",
    "    for train_index, test_index in split.split(data_x, data_y):\n",
    "        strat_train_set_x, strat_train_set_y = data_x.loc[train_index], data_y.loc[train_index]\n",
    "        strat_test_set_x, strat_test_set_y = data_x.loc[test_index], data_y.loc[test_index]\n",
    "\n",
    "        # Perform grid search to tune hyperparameters\n",
    "        param_grid = {\n",
    "            \"clf__n_estimators\": [100, 500, 1000],\n",
    "            \"clf__max_depth\": [1, 5, 10, 25],\n",
    "            \"clf__max_features\": [*numpy.arange(0.1, 1.1, 0.1)],\n",
    "        }\n",
    "\n",
    "        pipe = clone(pipeline)\n",
    "\n",
    "        pipe.fit(strat_train_set_x, strat_train_set_y)\n",
    "        pipe_predict_y = pipe.predict(strat_test_set_x)\n",
    "\n",
    "        auc = metrics.roc_auc_score(strat_test_set_y, pipe_predict_y)\n",
    "\n",
    "        current_fold = current_fold + 1\n",
    "\n",
    "        output = output.append(pandas.DataFrame({\n",
    "            \"fold\": [current_fold],\n",
    "            \"guessed \" + name  + \" precision score\": [precision_score(strat_test_set_y, pipe_predict_y)],\n",
    "            \"guessed \" + name  + \" recall score\": [recall_score(strat_test_set_y, pipe_predict_y)],\n",
    "            \"guessed not \" + name  + \" precision score\": [precision_score(negation(strat_test_set_y), negation(pipe_predict_y))],\n",
    "            \"guessed not \" + name  + \" recall score\": [recall_score(negation(strat_test_set_y), negation(pipe_predict_y))],\n",
    "        }))\n",
    "    \n",
    "    output = output[1:]\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in split.split(data, data[CORRECT_COLUMN]):\n",
    "\n",
    "        pipe = clone(pipeline)\n",
    "        \n",
    "        pipe.fit(strat_train_set_x, strat_train_set_y)\n",
    "        pipe_predict_y = pipe.predict(strat_test_set_x)\n",
    "\n",
    "        auc = metrics.roc_auc_score(strat_test_set_y, pipe_predict_y)\n",
    "\n",
    "        current_fold = current_fold + 1\n",
    "\n",
    "        output = output.append(pandas.DataFrame({\n",
    "            \"fold\": [\"full dataset\" + str(current_fold)],\n",
    "            \"guessed \" + name  + \" precision score\": [precision_score(strat_test_set_y, pipe_predict_y)],\n",
    "            \"guessed \" + name  + \" recall score\": [recall_score(strat_test_set_y, pipe_predict_y)],\n",
    "            \"guessed not \" + name  + \" precision score\": [precision_score(negation(strat_test_set_y), negation(pipe_predict_y))],\n",
    "            \"guessed not \" + name  + \" recall score\": [recall_score(negation(strat_test_set_y), negation(pipe_predict_y))],\n",
    "        }))\n",
    "\n",
    "    output.to_json(output_file_path, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(name, input_file_path, output_file_path):\n",
    "        \n",
    "    data = pandas.read_json(input_file_path)\n",
    "    output = data[CORRECT_COLUMN]\n",
    "\n",
    "    total_records = data[data[VALID_COLUMN] == 1][VALID_COLUMN].sum()\n",
    "\n",
    "    data = remove_non_binary_columns(data)\n",
    "    data = data.drop(columns=[CORRECT_COLUMN])\n",
    "\n",
    "    numerical_data = numpy.stack([data[col].values for col in data.columns], 1)\n",
    "    numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
    "\n",
    "    output = torch.tensor(output.values).flatten()\n",
    "\n",
    "    test_records = int(total_records * .2)\n",
    "\n",
    "    numerical_train_data = numerical_data[:total_records-test_records]\n",
    "    categorical_train_data = pandas.DataFrame()\n",
    "    numerical_test_data = numerical_data[total_records-test_records:total_records]\n",
    "    train_outputs = output[:total_records-test_records]\n",
    "    test_outputs = output[total_records-test_records:total_records]\n",
    "\n",
    "    class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, num_numerical_cols, output_size, layers, p=0.4):\n",
    "            super().__init__()\n",
    "            self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "            all_layers = []\n",
    "            input_size = num_numerical_cols\n",
    "\n",
    "            for i in layers:\n",
    "                all_layers.append(nn.Linear(input_size, i))\n",
    "                all_layers.append(nn.ReLU())\n",
    "                input_size = i\n",
    "            all_layers.append(nn.Softmax())\n",
    "            self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        def forward(self, x_numerical):\n",
    "            x_numerical = self.batch_norm_num(x_numerical)\n",
    "            x = torch.tensor(x_numerical)\n",
    "            x = self.layers(x)\n",
    "            return x\n",
    "\n",
    "    model = Model(numerical_data.shape[1], 2, [200,100,50], p=0.4)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 1000\n",
    "    aggregated_losses = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_pred = model(numerical_train_data)\n",
    "        single_loss = loss_function(y_pred, train_outputs)\n",
    "        aggregated_losses.append(single_loss)\n",
    "\n",
    "        # if i%25 == 1:\n",
    "        #     print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_val = model(numerical_test_data)\n",
    "        loss = loss_function(y_val, test_outputs)\n",
    "    y_val = numpy.argmax(y_val, axis=1)\n",
    "    print(y_val)\n",
    "\n",
    "    output = pandas.DataFrame({\n",
    "            \"guessed \" + name  + \" precision score\": [precision_score(y_val, test_outputs)],\n",
    "            \"guessed \" + name  + \" recall score\": [recall_score(y_val, test_outputs)],\n",
    "            \"guessed not \" + name  + \" precision score\": [precision_score(negation(y_val), negation(test_outputs))],\n",
    "            \"guessed not \" + name  + \" recall score\": [recall_score(negation(y_val), negation(test_outputs))],\n",
    "        })\n",
    "    output.to_json(output_file_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_csv(input_file_path):\n",
    "    data = pandas.read_json(input_file_path)\n",
    "    data.to_csv(os.path.splitext(input_file_path)[0]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(name, is_correct, is_valid):\n",
    "    print(name, \"*********************\")\n",
    "    folder_path = \"../output/\" + name.lower() + \"/\"\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "    # Define the input file paths of the needed data. These should be in json format\n",
    "    PROBLEMS_INPUT_FILE_PATH = '../input/draw.json'\n",
    "    CHATGPT_INPUT_FILE_PATH = '../input/chatgpt.json'\n",
    "    EXTRACT_FEATURES_OUTPUT_FILE_PATH = folder_path + '1_extract_features.json'\n",
    "    COMBINE_OUTPUT_FILE_PATH = folder_path + '2_combine.json'\n",
    "    CAUSALITY_OUTPUT_FILE_PATH = folder_path + '3_causality.json'\n",
    "    CHATGPT_STATS_OUTPUT_FILE_PATH = folder_path + '4_chatgpt_stats.json'\n",
    "    SPLIT_DATASET_OUTPUT_FILE_PATH = folder_path + '5_split_dataset.json'\n",
    "    \n",
    "    extract_features(PROBLEMS_INPUT_FILE_PATH, EXTRACT_FEATURES_OUTPUT_FILE_PATH)\n",
    "    combine(EXTRACT_FEATURES_OUTPUT_FILE_PATH, CHATGPT_INPUT_FILE_PATH, COMBINE_OUTPUT_FILE_PATH, is_correct, is_valid)\n",
    "    causality_values(COMBINE_OUTPUT_FILE_PATH, CAUSALITY_OUTPUT_FILE_PATH)\n",
    "    chatgpt_stats(CHATGPT_INPUT_FILE_PATH, CHATGPT_STATS_OUTPUT_FILE_PATH)\n",
    "\n",
    "    convert_json_to_csv(CAUSALITY_OUTPUT_FILE_PATH)\n",
    "    convert_json_to_csv(CHATGPT_STATS_OUTPUT_FILE_PATH)\n",
    "\n",
    "    RANDOM_FOREST_OUTPUT_FILE_PATH = folder_path + '6_random_forest_50-50.json'\n",
    "    NEURAL_NETWORK_OUTPUT_FILE_PATH = folder_path + '7_neural_network_50-50.json'\n",
    "    split_dataset(COMBINE_OUTPUT_FILE_PATH, SPLIT_DATASET_OUTPUT_FILE_PATH, percent_correct=0.5)\n",
    "    random_forest(name, SPLIT_DATASET_OUTPUT_FILE_PATH, RANDOM_FOREST_OUTPUT_FILE_PATH)\n",
    "    neural_network(name, SPLIT_DATASET_OUTPUT_FILE_PATH, NEURAL_NETWORK_OUTPUT_FILE_PATH)\n",
    "    convert_json_to_csv(RANDOM_FOREST_OUTPUT_FILE_PATH)\n",
    "    convert_json_to_csv(NEURAL_NETWORK_OUTPUT_FILE_PATH)\n",
    "\n",
    "    RANDOM_FOREST_OUTPUT_FILE_PATH = folder_path + '6_random_forest_15-85.json'\n",
    "    NEURAL_NETWORK_OUTPUT_FILE_PATH = folder_path + '7_neural_network_15-85.json'\n",
    "    split_dataset(COMBINE_OUTPUT_FILE_PATH, SPLIT_DATASET_OUTPUT_FILE_PATH, percent_correct=0.15)\n",
    "    random_forest(name, SPLIT_DATASET_OUTPUT_FILE_PATH, RANDOM_FOREST_OUTPUT_FILE_PATH)\n",
    "    neural_network(name, SPLIT_DATASET_OUTPUT_FILE_PATH, NEURAL_NETWORK_OUTPUT_FILE_PATH)\n",
    "    convert_json_to_csv(RANDOM_FOREST_OUTPUT_FILE_PATH)\n",
    "    convert_json_to_csv(NEURAL_NETWORK_OUTPUT_FILE_PATH)\n",
    "\n",
    "    # CLEANUP INTERMEDIATE FILES\n",
    "    os.remove(EXTRACT_FEATURES_OUTPUT_FILE_PATH)\n",
    "    os.remove(COMBINE_OUTPUT_FILE_PATH)\n",
    "    os.remove(SPLIT_DATASET_OUTPUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not_Somewhat_Or_Fully_Correct *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nocet\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "C:\\Users\\nocet\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\nocet\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nocet\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\nocet\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Not_Fully_Correct *********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nocet\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nocet\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "C:\\Users\\nocet\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\nocet\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nocet\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\nocet\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nocet\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# is_valid --\n",
    "# Checks to see if a row is \"valid\". We define what's \"valid\" here\n",
    "# INPUT: [row] is a dict\n",
    "def is_valid(row):\n",
    "    return row[COLUMN_TO_CHECK] != INVALID\n",
    "\n",
    "def column_check(correct_solutions, row):\n",
    "    if row[COLUMN_TO_CHECK] in correct_solutions: return 0\n",
    "    else: return 1\n",
    "\n",
    "def not_somewhat_or_fully_correct(row):\n",
    "    return column_check([ALL_ANSWERS, ALL_ANSWERS_ROUNDED, SOME_SOLUTION, SOME_SOLUTION_ROUNDED], row)\n",
    "    \n",
    "def not_fully_correct(row):\n",
    "    return column_check([ALL_ANSWERS, ALL_ANSWERS_ROUNDED], row)\n",
    "\n",
    "execute(\"Not_Somewhat_Or_Fully_Correct\", not_somewhat_or_fully_correct, is_valid)\n",
    "execute(\"Not_Fully_Correct\", not_fully_correct, is_valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8d1748df71eecf34f17224acf051d3287a3072256bd15edcac33d40f9211192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
